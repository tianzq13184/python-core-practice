{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikit-learn Practice Problems\n",
        "\n",
        "This notebook contains practice problems covering essential scikit-learn operations for machine learning.\n",
        "\n",
        "**Instructions:**\n",
        "- Complete the code in each cell marked with `# TODO`\n",
        "- Run the cell to verify your solution matches the expected output\n",
        "- Each problem focuses on specific scikit-learn concepts for building ML pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Pipeline 思维\n",
        "\n",
        "### Problem 1.1: Preprocess + Model + Eval Pipeline\n",
        "Create a complete pipeline: preprocessing, model training, and evaluation.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Pipeline created: True\n",
        "Accuracy: > 0.8\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Create pipeline with preprocessing and model\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Pipeline created: True\")\n",
        "# print(f\"Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: 数据切分\n",
        "\n",
        "### Problem 2.1: Train/Valid/Test Split\n",
        "Split data into train, validation, and test sets.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Train: 60%, Valid: 20%, Test: 20%\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# TODO: Split into train/valid/test\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Train: {train_pct:.0%}, Valid: {valid_pct:.0%}, Test: {test_pct:.0%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2.2: Cross Validation (Concept)\n",
        "Use cross-validation to evaluate model performance.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Cross-validation scores: [0.85, 0.87, 0.86, 0.88, 0.85]\n",
        "Mean CV score: 0.86\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# TODO: Perform cross-validation\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Cross-validation scores: {cv_scores}\")\n",
        "# print(f\"Mean CV score: {mean_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: 特征处理\n",
        "\n",
        "### Problem 3.1: Encoding\n",
        "Encode categorical features using different methods (LabelEncoder, OneHotEncoder).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Categorical features encoded: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "categories = ['red', 'blue', 'green', 'red', 'blue', 'green']\n",
        "\n",
        "# TODO: Encode categorical features\n",
        "# Your code here\n",
        "\n",
        "# print(\"Categorical features encoded: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3.2: Standardization\n",
        "Standardize features using StandardScaler and MinMaxScaler.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Features standardized: True\n",
        "Mean: 0.0, Std: 1.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# TODO: Standardize features\n",
        "# Your code here\n",
        "\n",
        "# print(\"Features standardized: True\")\n",
        "# print(f\"Mean: {mean:.1f}, Std: {std:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3.3: Missing Value Handling\n",
        "Handle missing values in features using SimpleImputer.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Missing values handled: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
        "\n",
        "# TODO: Handle missing values\n",
        "# Your code here\n",
        "\n",
        "# print(\"Missing values handled: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3.4: Data Leakage Risk\n",
        "Demonstrate understanding of data leakage (e.g., fitting scaler on test data).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Data leakage avoided: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Demonstrate correct way to avoid data leakage\n",
        "# Your code here\n",
        "\n",
        "# print(\"Data leakage avoided: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: 常用模型\n",
        "\n",
        "### Problem 4.1: Linear Model\n",
        "Train and evaluate a linear model (LogisticRegression for classification).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Linear model accuracy: > 0.7\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Train linear model\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Linear model accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 4.2: Tree Model\n",
        "Train and evaluate a tree model (DecisionTreeClassifier).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Tree model accuracy: > 0.8\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Train tree model\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Tree model accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 4.3: Ensemble Model\n",
        "Train and evaluate an ensemble model (RandomForestClassifier).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Ensemble model accuracy: > 0.85\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Train ensemble model\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Ensemble model accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: 评估指标\n",
        "\n",
        "### Problem 5.1: Classification Metrics\n",
        "Calculate classification metrics: accuracy, precision, recall, F1-score.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Accuracy: 0.85, Precision: 0.83, Recall: 0.87, F1: 0.85\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# TODO: Calculate classification metrics\n",
        "# Your code here\n",
        "\n",
        "# print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 5.2: Regression Metrics\n",
        "Calculate regression metrics: MSE, MAE, R².\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "MSE: < 1.0, MAE: < 0.8, R²: > 0.9\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# TODO: Calculate regression metrics\n",
        "# Your code here\n",
        "\n",
        "# print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 5.3: Threshold and Imbalanced Data\n",
        "Handle imbalanced classification using threshold adjustment and class weights.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Imbalanced data handled: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Handle imbalanced data\n",
        "# Your code here\n",
        "\n",
        "# print(\"Imbalanced data handled: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: 过拟合与调参\n",
        "\n",
        "### Problem 6.1: Regularization\n",
        "Apply regularization to prevent overfitting (L1/L2 in LogisticRegression).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Regularization applied: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Apply regularization (L1 and L2)\n",
        "# Your code here\n",
        "\n",
        "# print(\"Regularization applied: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 6.2: Grid Search\n",
        "Use GridSearchCV to find best hyperparameters.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Best parameters found: True\n",
        "Best score: > 0.8\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Use GridSearchCV\n",
        "# Your code here\n",
        "\n",
        "# print(\"Best parameters found: True\")\n",
        "# print(f\"Best score: {best_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 6.3: Random Search\n",
        "Use RandomizedSearchCV for hyperparameter tuning (concept).\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Random search completed: True\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Use RandomizedSearchCV\n",
        "# Your code here\n",
        "\n",
        "# print(\"Random search completed: True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: 可解释与诊断\n",
        "\n",
        "### Problem 7.1: Feature Importance\n",
        "Extract and visualize feature importance from tree-based models.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Feature importance extracted: True\n",
        "Top feature: feature_0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Extract feature importance\n",
        "# Your code here\n",
        "\n",
        "# print(\"Feature importance extracted: True\")\n",
        "# print(f\"Top feature: {top_feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 7.2: Error Analysis\n",
        "Perform error analysis: identify misclassified samples and analyze patterns.\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "Error analysis completed: True\n",
        "Misclassification rate: < 0.2\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# TODO: Perform error analysis\n",
        "# Your code here\n",
        "\n",
        "# print(\"Error analysis completed: True\")\n",
        "# print(f\"Misclassification rate: {error_rate:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
